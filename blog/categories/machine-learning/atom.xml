<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine_learning | KING]]></title>
  <link href="http://pbking1.github.com/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://pbking1.github.com/"/>
  <updated>2014-05-20T20:03:11+08:00</updated>
  <id>http://pbking1.github.com/</id>
  <author>
    <name><![CDATA[pb]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Linear_regression_tutorial]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/12/linear-regression-tutorial/"/>
    <updated>2014-05-12T15:23:20+08:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/12/linear-regression-tutorial</id>
    <content type="html"><![CDATA[<h3>First something about training the data</h3>

<ul>
<li>Training set &ndash;> learning algorithm &ndash;> hypothesis &ndash;> estimated data</li>
<li>And we will use multiple features.

<ul>
<li><img src="/images/lr_gd/mfeat.png"></li>
</ul>
</li>
</ul>


<h3>What is linear regression</h3>

<ul>
<li>To say the idea in normal, we will have n feature</li>
<li>And what we are going to do is to observe them and suppose that they should be in a linear method.</li>
<li>And we should develop a way to find out the parameter to suit the hypothesis function.</li>
</ul>


<!--more-->


<h3>The first method to solve the problem(Gradient decend)</h3>

<h4>how it works?</h4>

<ul>
<li>There are several ideas

<ul>
<li><strong>learn rate</strong>

<ul>
<li>if too small, the gradient descent will be slow</li>
<li>if too large, the gradient descent will overshoot the minimum.

<ul>
<li>it may be fail to converge, or even diverge</li>
</ul>
</li>
<li><img src="/images/lr_gd/apha.png"></li>
</ul>
</li>
<li><strong>Cost function(J function)</strong>

<ul>
<li>The cost function has no relationship with the x and y.</li>
<li>and it all depend on the parameter <em>theta</em></li>
<li>and the &frac12; in the front is to make the derivation easier</li>
<li>And the cost function is used to <em>make the err(the err between hypothesis and real data) smaller</em>.</li>
</ul>
</li>
<li><strong>Hypothesis</strong>

<ul>
<li>This function is used to <em>make the function good</em>.</li>
<li>good enough to make the y calculate by the function can came close to the real y.</li>
<li>and we will use the err of them to see whether it is good enough or not.</li>
<li>and with all the data, we sum them.</li>
</ul>
</li>
<li><strong>theta</strong>

<ul>
<li><p>we can see in the following picture</p>

<ul>
<li>suppose that we went down the mountain and we can see all the mountain around us are higher than us</li>
<li>and the theta in the cost function is that we <em>reduce</em> theta value in the theta direction</li>
<li>and the deviation of the cost function on theta i is the distance that we move on theta i direction.</li>
</ul>
</li>
<li><p><img src="/images/lr_gd/gdpic.png"></p></li>
</ul>
</li>
</ul>
</li>
<li><strong>Basic algorithm</strong>

<ul>
<li>start with some theta0, theta1, theta2&hellip;..</li>
<li>Keep changing theta0, theta1, theta2&hellip;. to reduce J(theta0, theta2&hellip;) until we hopefully end up at a minimum.</li>
<li>And this is the key idea of Gradient descend method</li>
</ul>
</li>
<li><img src="/images/lr_gd/idea.png"></li>
</ul>


<h3>The cpp code of it</h3>

<p>```</p>

<h1>include&lt;stdlib.h></h1>

<h1>include&lt;stdio.h></h1>

<h1>include&lt;string.h></h1>

<h1>include&lt;math.h></h1>

<h1>define OUTPUTID 10001</h1>

<h1>define BUFFERSIZE 50000</h1>

<h1>define ROWNUM 10000</h1>

<h1>define COLNUM 385</h1>

<p>double alpha = 0.1;
char buffer[BUFFERSIZE];
const char *delim = &ldquo;,&rdquo;;
double x[ROWNUM][COLNUM];
double y[ROWNUM];
double result[ROWNUM];
double diff[ROWNUM];
double theta[COLNUM];
double temp[COLNUM];</p>

<p>void readdata(char <em>, bool);
void writedata(char </em>);
void test();
void gradient_descend_train();</p>

<p>int main(){</p>

<pre><code>readdata("train.csv", true);
gradient_descend_train();
readdata("test.csv", false);
test();
writedata("predict.csv");
return 0;
</code></pre>

<p>}</p>

<p>void readdata(char *filename, bool haspredicted){</p>

<pre><code>FILE *inputfile = fopen(filename, "r");

if(inputfile == NULL){
    system("PAUSE");
    exit(1);
}
//drop the first line
fscanf(inputfile, "%s", buffer);
//read all lines each
char *s;
for(int i = 0; i &lt; ROWNUM; i++){

    fscanf(inputfile, "%s", buffer);
    //drop the first column
    strtok(buffer, delim);
    //read the predict y
    if(haspredicted){
        s = strtok(NULL, delim);
        sscanf(s, "%lf", &amp;y[i]);
    }
    //init x0
    x[i][0] = 1;
    //read the matrix
    for(int j = 1; j &lt; COLNUM; j++){
        s = strtok(NULL, delim);
        sscanf(s, "%lf", &amp;x[i][j]);
    }
}
fclose(inputfile);
</code></pre>

<p>}</p>

<p>void writedata(char *filename){</p>

<pre><code>FILE *outputfile = fopen(filename, "w");

if(outputfile == NULL){
    system("pause");
    exit(1);
}

fprintf(outputfile, "%s,%s\n", "Id", "reference");
//write the result into file
for(int i = 0, id = OUTPUTID; i &lt; ROWNUM; i++, id++){
    //cout&lt;&lt;"write the line"&lt;&lt;i + 1&lt;&lt;endl;
    fprintf(outputfile, "%d,%.6lf\n", id, result[i]);
}
fclose(outputfile);
</code></pre>

<p>}</p>

<p>void initTheta(){  //init theta</p>

<pre><code>char *thetafilename = "theta.dat";
FILE *f = fopen(thetafilename, "r");
for(int j = 0; j &lt; COLNUM; j++)
    fscanf(f, "%lf", &amp;theta[j]);
fclose(f);
//init the theta
for(int j = 0; j &lt; COLNUM; j++)
    theta[j] = 0;
</code></pre>

<p>}</p>

<p>void saveTheta(){   //save the theta</p>

<pre><code>FILE *f = fopen("theta.dat", "w");
for(int j = 0; j &lt; COLNUM; j++)
    fprintf(f, "%lf\n", theta[j]);
fclose(f);
</code></pre>

<p>}</p>

<p>void calculateResult(){</p>

<pre><code>for(int i = 0; i &lt; ROWNUM; i++){
    result[i] = 0;
    for(int j = 0; j &lt; COLNUM; j++){
        result[i] += theta[j] * x[i][j];
    }
}
</code></pre>

<p>}</p>

<p>double calculateJ(){</p>

<pre><code>int turn = 0;
double cost = 0;
for(int i = 0; i &lt; ROWNUM; i++){
    diff[i] = result[i] - y[i];
    cost += diff[i]*diff[i];
}
cost /= (ROWNUM * 2);
printf("%5d: J(theta) = %.6lf\n", ++turn, cost);
return cost;
</code></pre>

<p>}</p>

<p>void updateTheta(){</p>

<pre><code>double sum;
for(int j = 0 ; j &lt; COLNUM; j++){
    sum = 0;
    for(int i = 0; i &lt; ROWNUM; i++)
        sum += diff[i] * x[i][j];
    theta[j] -= alpha * sum / ROWNUM;
}
</code></pre>

<p>}</p>

<p>void gradient_descend_train(){</p>

<pre><code>initTheta();
alpha = 0.1001;
double cost = 1000;
while(cost &gt; 26.4){
    calculateResult();
    cost = calculateJ();
    updateTheta();
}
saveTheta();
</code></pre>

<p>}</p>

<p>void test(){</p>

<pre><code>calculateResult();
</code></pre>

<p>}</p>

<p>```</p>

<h3>The Second method to solve the problem(normal equation)</h3>

<ul>
<li>This way is a way shown in the statistic learning.</li>
<li>use the minimum square function to do a regression analyse on the data.

<ul>
<li><img src="/images/lr_gd/nq.png"></li>
<li>and the process is shown : (385<em>10000)</em>(10000<em>385)</em>(385<em>10000)</em>10000<em>1=385</em>1</li>
</ul>
</li>
<li>and we can see the feature normalise in the normal equation function

<ul>
<li><img src="/images/lr_gd/nfn.png"></li>
</ul>
</li>
</ul>


<h3>The matlab code of it</h3>

<ul>
<li>The main function</li>
</ul>


<p>```
%load the train data
data = load(&lsquo;train.txt&rsquo;);
X = data(:, 3:386);
y = data(:, 2);
m = length(y);
m2 = size(X);</p>

<p>%load the test data
data2 = load(&lsquo;test.txt&rsquo;);
feat = data2(:, 2:385);
m3 = size(feat);</p>

<p>sum_test = [0];</p>

<p>%use the equation to calculate
theta = normaleqn(X, y, w);</p>

<p>%calculate the result
result = feat * theta;</p>

<p>csvwrite(&lsquo;aaa_ver3.csv&rsquo;, [linen result]);
```</p>

<ul>
<li>The normal equation function</li>
</ul>


<p>```
function [theta] = normaleqn(x, y, w)</p>

<pre><code>theta = zeros(size(x, 2), 1);
%theta = pinv(x' * x + 4000.3 * eye(size(x, 2))) * x' * y;
 %theta = pinv(x' * x + 3.3 * eye(size(x, 2))) * x' * y;
theta = pinv(x' * x + w * eye(size(x, 2))) * x' * y;
</code></pre>

<p>en
```</p>

<h3>The cpp code of it</h3>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Difference Between Data Mining and Machine Learning]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/10/the-difference-between-data-mining-and-machine-learning/"/>
    <updated>2014-05-10T00:00:26+08:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/10/the-difference-between-data-mining-and-machine-learning</id>
    <content type="html"><![CDATA[<h3>What is machine learning?</h3>

<ul>
<li>Machine learning is is a system that decide the system what to do next

<ul>
<li>but also <strong>improve the system with the answer that exist</strong></li>
</ul>
</li>
<li>It is focus more on a feedback framework.

<ul>
<li>That is to say, the system may be not very smart at first

<ul>
<li>but it may be smarter after training</li>
</ul>
</li>
</ul>
</li>
<li>What&rsquo;s more, this area is one of the most important part in AI.

<ul>
<li>And the first aim to develop this method is that the scientist want to make the computer system ot have enough intelligence to develop AI.</li>
</ul>
</li>
</ul>


<!--more-->


<h3>what is data mining?</h3>

<ul>
<li>Data mining a kind of science that develop through the Internet.

<ul>
<li>And data mining is focusing more on the interaction between database and learning algorithm</li>
</ul>
</li>
<li>And that&rsquo;s why Google is so strong.</li>
<li>And what data mining do is to find the <strong>relation</strong> ship of the phrase in the data.</li>
</ul>


<h3>addition what is AI?</h3>

<ul>
<li>AI is called Artificial Intelligence</li>
<li>And I think that AI should contain the following component

<ul>
<li>Pattern recognition</li>
<li>Data mining</li>
<li>Machine learning</li>
</ul>
</li>
<li>That is to say

<ul>
<li>every thing that is in the area <strong>making decision</strong> is AI.</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tree Based Ensemble Method Introduction]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/08/tree-based-ensemble-method-introduction/"/>
    <updated>2014-05-08T18:30:07+08:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/08/tree-based-ensemble-method-introduction</id>
    <content type="html"><![CDATA[<h3>Tree-based ensemble methods</h3>

<pre><code>random forest
gradient boosted decision trees
</code></pre>

<ul>
<li>main idea is greedy algorithm

<ul>
<li>通过构造一棵决策树分类器</li>
</ul>
</li>
<li>随机森林是通过构造10000课树</li>
</ul>


<!--more-->


<h4>决策树</h4>

<ul>
<li>其实就是二叉树</li>
<li>实际应用，一般为二叉树</li>
<li>每个非叶子节点都是一个分割

<ul>
<li>相当于一个分类条件</li>
</ul>
</li>
<li>每个叶子节点都是一类性质相同的样本</li>
<li><p>用同一种数据存在多种构造决策树的情况</p></li>
<li><p>决策树使用来分类的模型</p>

<ul>
<li>相当于svn和LR</li>
</ul>
</li>
<li>如何建树

<ul>
<li>是一种回归分类算法

<ul>
<li>“切分”和“解决”</li>
</ul>
</li>
<li>一开始的时候，所有的训练样本都在根部</li>
<li>然后分类的样本将基于选择的分类属性进行递归。</li>
</ul>
</li>
<li>理论上特征是可以重复选的</li>
<li>怎么找到比较好的树？

<ul>
<li>模型准确率高</li>
<li>找到尽可能最小的数来满足数据

<ul>
<li>分枝少</li>
<li>节点少</li>
</ul>
</li>
<li>occam&rsquo;s Racor：在效果超不多的时候，选比较简单的模型</li>
</ul>
</li>
<li>基本策略

<ul>
<li>贪心

<ul>
<li>当前把每个节点中的最优的特征作为分类标准</li>
<li>这样就不用穷举</li>
<li>评估

<ul>
<li>基尼系数（Gini index）</li>
<li>信息增益

<ul>
<li>用熵来衡量</li>
<li>混乱程度

<ul>
<li>如果容易分别出那个分类多，则混乱程度小</li>
<li>反之同理</li>
</ul>
</li>
<li>e.g 99/1（小） 50/50（大）</li>
</ul>
</li>
<li>则把每个叶子节点的熵都算出来

<ul>
<li>然后整棵树的熵则是用所有叶子节点的熵加权</li>
</ul>
</li>
<li>把分割前的熵减掉分割之后的，就是信息增益

<ul>
<li>如果比较大，那么就比较好</li>
</ul>
</li>
</ul>
</li>
<li> Gini index

<ul>
<li> 把叶子节点的熵换掉</li>
<li> 换成基尼系数</li>
<li> 基本上也是混乱度</li>
<li> 只是公式不一样了</li>
</ul>
</li>
<li> 整棵树的Gini系数是加权的分支的基尼</li>
<li> 如何防止overfitting？

<ul>
<li> 早一点停止树的增长</li>
<li> 先把所有节点来构造树，然后用的是90%的样本

<ul>
<li> 然后用剩下的样本来检验效果</li>
<li> 但是太慢了</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Bagging

<ul>
<li>构造k棵树</li>
<li>每棵树在1000各样本中随机选900个,构造决策树（只筛选样本）

<ul>
<li>然后把所有树的据结果统计，如果是分类问题，就少数服从多数。</li>
<li>随机选出的数据室可以重复的。</li>
</ul>
</li>
<li>如果是回归问题，则最终直接相加</li>
</ul>
</li>
<li><p>random forest</p>

<ul>
<li>不使用全部特征</li>
<li>不单只筛选样本，也筛选特征。</li>
<li>e.g 每次只用随机抽取的70%的样本，只用50%随机抽取的特征，来建立决策树</li>
<li>是目前分类回归中最好的off-the-shelf的算法

<ul>
<li>即拿即用</li>
</ul>
</li>
</ul>
</li>
<li><p>这两个算法加上boosting的不同在于训练集的构造的不同。</p></li>
<li>Boosting

<ul>
<li>结果是把每棵决策树的结果加权加起来</li>
<li>所有样本的权值加起来为1</li>
<li>对那些错的样本，权值是比较大的</li>
<li>缺点是在高噪声的环境下效果很不好

<ul>
<li>优点是在低噪声环境下效果很好</li>
</ul>
</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
</feed>
