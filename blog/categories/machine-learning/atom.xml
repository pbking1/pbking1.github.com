<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine_learning | KING]]></title>
  <link href="http://pbking1.github.com/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://pbking1.github.com/"/>
  <updated>2014-05-27T13:24:38+08:00</updated>
  <id>http://pbking1.github.com/</id>
  <author>
    <name><![CDATA[pb]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[KNN in Matlab(knnclassify)]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/26/knn-in-matlab-knnclassify/"/>
    <updated>2014-05-26T17:55:05+08:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/26/knn-in-matlab-knnclassify</id>
    <content type="html"><![CDATA[<ul>
<li>refer from

<ul>
<li><a href="http://blog.csdn.net/boyxiaolong/article/details/7062394">http://blog.csdn.net/boyxiaolong/article/details/7062394</a></li>
<li><a href="http://blog.csdn.net/aladdina/article/details/4141127">http://blog.csdn.net/aladdina/article/details/4141127</a></li>
</ul>
</li>
</ul>


<h3>What is KNN</h3>

<ul>
<li>simlpy, a method use to classify or regression</li>
<li>idea

<ul>
<li>如果一个样本在特征空间中的k个最相 似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</li>
</ul>
</li>
<li>KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成正比。</li>
</ul>


<!--more-->


<h3><code>knnclassify</code> in matlab</h3>

<ul>
<li>sample
```
train_data = load(&lsquo;train.csv&rsquo;);
test_data = load(&lsquo;test.csv&rsquo;);</li>
</ul>


<p>X = train_data(:, 3:3074);
Y = train_data(:, 2);
x = test_data(:, 2:3073);</p>

<p>label = knnclassify(x,X,Y,10,&lsquo;cosine&rsquo;,&lsquo;random&rsquo;);
```
&ndash; knnclassify(test_data,train_data,train_label,numberoflabel,&lsquo;cosine&rsquo;,&lsquo;random&rsquo;);</p>

<h3>main function</h3>

<ul>
<li>1.CLASS = KNNCLASSIFY(SAMPLE,TRAINING,GROUP)

<ul>
<li>标号和训练数据必须有相同的行数；训练数据和测试数据必须有相同的列；函数对于无效值或者空值会作为丢失值或者忽略这一行。</li>
</ul>
</li>
<li>2.CLASS = KNNCLASSIFY(SAMPLE,TRAINING,GROUP,K)

<ul>
<li>此函数允许你设置距离矩阵形式，如：

<ul>
<li>&lsquo;euclidean&rsquo;    欧氏距离，默认的</li>
<li>&lsquo;cityblock&rsquo;    绝对差的和</li>
<li>&lsquo;cosine&rsquo;     角度距离</li>
<li>&lsquo;correlation&rsquo; 相关距离</li>
<li>&lsquo;Hamming&rsquo;      汉明距离</li>
</ul>
</li>
</ul>
</li>
<li>3.CLASS =KNNCLASSIFY(SAMPLE,TRAINING,GROUP,K,DISTANCE,RULE)

<ul>
<li>本函数允许你选择如何对样本进行分类，如你可以选择：

<ul>
<li>&lsquo;nearest&rsquo;  最近的K个的最多数</li>
<li>&lsquo;random&rsquo;    随机的最多数</li>
<li>&lsquo;consensus&rsquo; 投票法，默认的</li>
</ul>
</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intro to Recommandation System]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/24/intro-to-recommandation-system/"/>
    <updated>2014-05-24T10:32:15+08:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/24/intro-to-recommandation-system</id>
    <content type="html"><![CDATA[<h3>Problem formulation</h3>

<pre><code>可能会有99%的数据是不知道的（用问号表示）
因此可以去猜那些不知道的数据是多少
然后把得分最高的电影推荐给他就可以了
所以其实是一个矩阵补全系统
</code></pre>

<!--more-->


<h3>基本思想和方法</h3>

<ul>
<li><p>基于人口统计学的推荐系统</p>

<ul>
<li>最简单的一种，只是根据系统用户的基本信息发现用户的相关程度，然后把相似用户喜爱的其他用品推荐给当前用户。</li>
<li>系统首先会根据用户的属性建模，比如用户的年龄，性别，兴趣等信息。根据这些特征计算用户间的相似度。比如系统通过计算发现用户A和C比较相似。就会把A喜欢的物品推荐给C。

<ul>
<li>优势

<ul>
<li>不需要历史数据，没有冷启动问题</li>
<li>不依赖于物品的属性，因此其他领域的问题都可无缝接入。</li>
</ul>
</li>
<li>不足：

<ul>
<li>算法比较粗糙，效果很难令人满意，只适合简单的推荐</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>content based recommender system(基于内容)</p>

<ul>
<li>使用物品本身的相似度而不是用户的相似度。</li>
<li>e.g 通过相似度计算，发现电影A和C相似度较高，因为他们都属于爱情类。系统还会发现用户A喜欢电影A，由此得出结论，用户A很可能对电影C也感兴趣。于是将电影C推荐给A。</li>
<li>假设有一些别的特征例如电影的浪漫成都或者动作成分含量（由专家弄出来的）</li>
<li>因此可以把原有的评分作为输入，把专家的预测作为y

<ul>
<li>做线性回归， 把每个用户的theta学出来</li>
<li>然后把theta和用户的评分做内积，得出那些问号的数据</li>
</ul>
</li>
</ul>
</li>
<li><p>协同过滤</p>

<ul>
<li>基于物品的协同过滤

<ul>
<li>根本思想是

<ul>
<li>预先根据所有用户的历史偏好数据计算物品之间的相似性，然后把和用户喜欢的物品相类似的物品推荐给用户。</li>
<li>假设a和c很相近，因为喜欢a的用户同时也喜欢c，而用户A喜欢a，所以把c推荐给用户A</li>
</ul>
</li>
</ul>
</li>
<li><p>如何计算相似度</p>

<ul>
<li>基于余弦的相似度计算

<ul>
<li>通过计算两个向量之间的夹角的余弦值来计算物品之间的相似性</li>
<li>公式为

<ul>
<li><img src="/images/recommander_system/1.png"></li>
</ul>
</li>
<li>这个算法也有改进版（修正的余弦相似性）

<ul>
<li>由于余弦相似度度量方法中没有考虑到不同用户的评分尺寸问题，修正的余弦相似性度量方法通过减去用户对项目的平均评分来改善上述缺陷。</li>
<li>公式为

<ul>
<li><img src="/images/recommander_system/2.png"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>基于关联的相似度计算

<ul>
<li>计算两个向量之间的Pearson-r关联度</li>
</ul>
</li>
<li>调整的余弦相似度计算

<ul>
<li>由于基于余弦的相似度计算没有考虑不同用户的打分情况，可能有的用户偏向于给低分，有的用户偏向于给高分，该方法通过减去用户的打分的平均值消除不同用户打分习惯的影响</li>
</ul>
</li>
</ul>
</li>
<li><p>预测值的计算</p>

<ul>
<li>根据之前计算好的物品之间的相似度，接下来对用户未打分的物品进行预测，有两种预测方法。

<ul>
<li>1.加权求和

<ul>
<li>通过公式

<ul>
<li><img src="/images/recommander_system/3.png"></li>
</ul>
</li>
</ul>
</li>
<li>2.回归</li>
</ul>
</li>
</ul>
</li>
<li><p>推荐系统的其中一种实现</p>

<ul>
<li>1.建立物品的同现矩阵

<ul>
<li>也就是按照用户分组，找出每两个物品再多少个用户中同时出现的次数</li>
</ul>
</li>
<li>2.建立用户的评分矩阵

<ul>
<li>也就是每个用户对每个物品的评分</li>
</ul>
</li>
<li>3.两个矩阵相乘，计算结果</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>源码</h3>

<ul>
<li>to be continued&hellip;</li>
</ul>


<h3>参考文献</h3>

<ul>
<li>《基于项目评分预测的协同过滤推荐算法》 邓爱林</li>
<li>相似度计算 <a href="http://blog.sina.com.cn/s/blog_7e11a6260101l9iq.html">http://blog.sina.com.cn/s/blog_7e11a6260101l9iq.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linear_regression_tutorial]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/12/linear-regression-tutorial/"/>
    <updated>2014-05-12T15:23:20+08:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/12/linear-regression-tutorial</id>
    <content type="html"><![CDATA[<h3>First something about training the data</h3>

<ul>
<li>Training set &ndash;> learning algorithm &ndash;> hypothesis &ndash;> estimated data</li>
<li>And we will use multiple features.

<ul>
<li><img src="/images/lr_gd/mfeat.png"></li>
</ul>
</li>
</ul>


<h3>What is linear regression</h3>

<ul>
<li>To say the idea in normal, we will have n feature</li>
<li>And what we are going to do is to observe them and suppose that they should be in a linear method.</li>
<li>And we should develop a way to find out the parameter to suit the hypothesis function.</li>
</ul>


<!--more-->


<h3>The first method to solve the problem(Gradient decend)</h3>

<h4>how it works?</h4>

<ul>
<li>There are several ideas

<ul>
<li><strong>learn rate</strong>

<ul>
<li>if too small, the gradient descent will be slow</li>
<li>if too large, the gradient descent will overshoot the minimum.

<ul>
<li>it may be fail to converge, or even diverge</li>
</ul>
</li>
<li><img src="/images/lr_gd/apha.png"></li>
</ul>
</li>
<li><strong>Cost function(J function)</strong>

<ul>
<li>The cost function has no relationship with the x and y.</li>
<li>and it all depend on the parameter <em>theta</em></li>
<li>and the &frac12; in the front is to make the derivation easier</li>
<li>And the cost function is used to <em>make the err(the err between hypothesis and real data) smaller</em>.</li>
</ul>
</li>
<li><strong>Hypothesis</strong>

<ul>
<li>This function is used to <em>make the function good</em>.</li>
<li>good enough to make the y calculate by the function can came close to the real y.</li>
<li>and we will use the err of them to see whether it is good enough or not.</li>
<li>and with all the data, we sum them.</li>
</ul>
</li>
<li><strong>theta</strong>

<ul>
<li><p>we can see in the following picture</p>

<ul>
<li>suppose that we went down the mountain and we can see all the mountain around us are higher than us</li>
<li>and the theta in the cost function is that we <em>reduce</em> theta value in the theta direction</li>
<li>and the deviation of the cost function on theta i is the distance that we move on theta i direction.</li>
</ul>
</li>
<li><p><img src="/images/lr_gd/gdpic.png"></p></li>
</ul>
</li>
</ul>
</li>
<li><strong>Basic algorithm</strong>

<ul>
<li>start with some theta0, theta1, theta2&hellip;..</li>
<li>Keep changing theta0, theta1, theta2&hellip;. to reduce J(theta0, theta2&hellip;) until we hopefully end up at a minimum.</li>
<li>And this is the key idea of Gradient descend method</li>
</ul>
</li>
<li><img src="/images/lr_gd/idea.png"></li>
</ul>


<h3>The cpp code of it</h3>

<p>```</p>

<h1>include&lt;stdlib.h></h1>

<h1>include&lt;stdio.h></h1>

<h1>include&lt;string.h></h1>

<h1>include&lt;math.h></h1>

<h1>define OUTPUTID 10001</h1>

<h1>define BUFFERSIZE 50000</h1>

<h1>define ROWNUM 10000</h1>

<h1>define COLNUM 385</h1>

<p>double alpha = 0.1;
char buffer[BUFFERSIZE];
const char *delim = &ldquo;,&rdquo;;
double x[ROWNUM][COLNUM];
double y[ROWNUM];
double result[ROWNUM];
double diff[ROWNUM];
double theta[COLNUM];
double temp[COLNUM];</p>

<p>void readdata(char <em>, bool);
void writedata(char </em>);
void test();
void gradient_descend_train();</p>

<p>int main(){</p>

<pre><code>readdata("train.csv", true);
gradient_descend_train();
readdata("test.csv", false);
test();
writedata("predict.csv");
return 0;
</code></pre>

<p>}</p>

<p>void readdata(char *filename, bool haspredicted){</p>

<pre><code>FILE *inputfile = fopen(filename, "r");

if(inputfile == NULL){
    system("PAUSE");
    exit(1);
}
//drop the first line
fscanf(inputfile, "%s", buffer);
//read all lines each
char *s;
for(int i = 0; i &lt; ROWNUM; i++){

    fscanf(inputfile, "%s", buffer);
    //drop the first column
    strtok(buffer, delim);
    //read the predict y
    if(haspredicted){
        s = strtok(NULL, delim);
        sscanf(s, "%lf", &amp;y[i]);
    }
    //init x0
    x[i][0] = 1;
    //read the matrix
    for(int j = 1; j &lt; COLNUM; j++){
        s = strtok(NULL, delim);
        sscanf(s, "%lf", &amp;x[i][j]);
    }
}
fclose(inputfile);
</code></pre>

<p>}</p>

<p>void writedata(char *filename){</p>

<pre><code>FILE *outputfile = fopen(filename, "w");

if(outputfile == NULL){
    system("pause");
    exit(1);
}

fprintf(outputfile, "%s,%s\n", "Id", "reference");
//write the result into file
for(int i = 0, id = OUTPUTID; i &lt; ROWNUM; i++, id++){
    //cout&lt;&lt;"write the line"&lt;&lt;i + 1&lt;&lt;endl;
    fprintf(outputfile, "%d,%.6lf\n", id, result[i]);
}
fclose(outputfile);
</code></pre>

<p>}</p>

<p>void initTheta(){  //init theta</p>

<pre><code>char *thetafilename = "theta.dat";
FILE *f = fopen(thetafilename, "r");
for(int j = 0; j &lt; COLNUM; j++)
    fscanf(f, "%lf", &amp;theta[j]);
fclose(f);
//init the theta
for(int j = 0; j &lt; COLNUM; j++)
    theta[j] = 0;
</code></pre>

<p>}</p>

<p>void saveTheta(){   //save the theta</p>

<pre><code>FILE *f = fopen("theta.dat", "w");
for(int j = 0; j &lt; COLNUM; j++)
    fprintf(f, "%lf\n", theta[j]);
fclose(f);
</code></pre>

<p>}</p>

<p>void calculateResult(){</p>

<pre><code>for(int i = 0; i &lt; ROWNUM; i++){
    result[i] = 0;
    for(int j = 0; j &lt; COLNUM; j++){
        result[i] += theta[j] * x[i][j];
    }
}
</code></pre>

<p>}</p>

<p>double calculateJ(){</p>

<pre><code>int turn = 0;
double cost = 0;
for(int i = 0; i &lt; ROWNUM; i++){
    diff[i] = result[i] - y[i];
    cost += diff[i]*diff[i];
}
cost /= (ROWNUM * 2);
printf("%5d: J(theta) = %.6lf\n", ++turn, cost);
return cost;
</code></pre>

<p>}</p>

<p>void updateTheta(){</p>

<pre><code>double sum;
for(int j = 0 ; j &lt; COLNUM; j++){
    sum = 0;
    for(int i = 0; i &lt; ROWNUM; i++)
        sum += diff[i] * x[i][j];
    theta[j] -= alpha * sum / ROWNUM;
}
</code></pre>

<p>}</p>

<p>void gradient_descend_train(){</p>

<pre><code>initTheta();
alpha = 0.1001;
double cost = 1000;
while(cost &gt; 26.4){
    calculateResult();
    cost = calculateJ();
    updateTheta();
}
saveTheta();
</code></pre>

<p>}</p>

<p>void test(){</p>

<pre><code>calculateResult();
</code></pre>

<p>}</p>

<p>```</p>

<h3>The Second method to solve the problem(normal equation)</h3>

<ul>
<li>This way is a way shown in the statistic learning.</li>
<li>use the minimum square function to do a regression analyse on the data.

<ul>
<li><img src="/images/lr_gd/nq.png"></li>
<li>and the process is shown : (385<em>10000)</em>(10000<em>385)</em>(385<em>10000)</em>10000<em>1=385</em>1</li>
</ul>
</li>
<li>and we can see the feature normalise in the normal equation function

<ul>
<li><img src="/images/lr_gd/nfn.png"></li>
</ul>
</li>
</ul>


<h3>The matlab code of it</h3>

<ul>
<li>The main function</li>
</ul>


<p>```
%load the train data
data = load(&lsquo;train.txt&rsquo;);
X = data(:, 3:386);
y = data(:, 2);
m = length(y);
m2 = size(X);</p>

<p>%load the test data
data2 = load(&lsquo;test.txt&rsquo;);
feat = data2(:, 2:385);
m3 = size(feat);</p>

<p>sum_test = [0];</p>

<p>%use the equation to calculate
theta = normaleqn(X, y, w);</p>

<p>%calculate the result
result = feat * theta;</p>

<p>csvwrite(&lsquo;aaa_ver3.csv&rsquo;, [linen result]);
```</p>

<ul>
<li>The normal equation function</li>
</ul>


<p>```
function [theta] = normaleqn(x, y, w)</p>

<pre><code>theta = zeros(size(x, 2), 1);
%theta = pinv(x' * x + 4000.3 * eye(size(x, 2))) * x' * y;
 %theta = pinv(x' * x + 3.3 * eye(size(x, 2))) * x' * y;
theta = pinv(x' * x + w * eye(size(x, 2))) * x' * y;
</code></pre>

<p>en
```</p>

<h3>The cpp code of it</h3>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Difference Between Data Mining and Machine Learning]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/10/the-difference-between-data-mining-and-machine-learning/"/>
    <updated>2014-05-10T00:00:26+08:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/10/the-difference-between-data-mining-and-machine-learning</id>
    <content type="html"><![CDATA[<h3>What is machine learning?</h3>

<ul>
<li>Machine learning is is a system that decide the system what to do next

<ul>
<li>but also <strong>improve the system with the answer that exist</strong></li>
</ul>
</li>
<li>It is focus more on a feedback framework.

<ul>
<li>That is to say, the system may be not very smart at first

<ul>
<li>but it may be smarter after training</li>
</ul>
</li>
</ul>
</li>
<li>What&rsquo;s more, this area is one of the most important part in AI.

<ul>
<li>And the first aim to develop this method is that the scientist want to make the computer system ot have enough intelligence to develop AI.</li>
</ul>
</li>
</ul>


<!--more-->


<h3>what is data mining?</h3>

<ul>
<li>Data mining a kind of science that develop through the Internet.

<ul>
<li>And data mining is focusing more on the interaction between database and learning algorithm</li>
</ul>
</li>
<li>And that&rsquo;s why Google is so strong.</li>
<li>And what data mining do is to find the <strong>relation</strong> ship of the phrase in the data.</li>
</ul>


<h3>addition what is AI?</h3>

<ul>
<li>AI is called Artificial Intelligence</li>
<li>And I think that AI should contain the following component

<ul>
<li>Pattern recognition</li>
<li>Data mining</li>
<li>Machine learning</li>
</ul>
</li>
<li>That is to say

<ul>
<li>every thing that is in the area <strong>making decision</strong> is AI.</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tree Based Ensemble Method Introduction]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/08/tree-based-ensemble-method-introduction/"/>
    <updated>2014-05-08T18:30:07+08:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/08/tree-based-ensemble-method-introduction</id>
    <content type="html"><![CDATA[<h3>Tree-based ensemble methods</h3>

<pre><code>random forest
gradient boosted decision trees
</code></pre>

<ul>
<li>main idea is greedy algorithm

<ul>
<li>通过构造一棵决策树分类器</li>
</ul>
</li>
<li>随机森林是通过构造10000课树</li>
</ul>


<!--more-->


<h4>决策树</h4>

<ul>
<li>其实就是二叉树</li>
<li>实际应用，一般为二叉树</li>
<li>每个非叶子节点都是一个分割

<ul>
<li>相当于一个分类条件</li>
</ul>
</li>
<li>每个叶子节点都是一类性质相同的样本</li>
<li><p>用同一种数据存在多种构造决策树的情况</p></li>
<li><p>决策树使用来分类的模型</p>

<ul>
<li>相当于svn和LR</li>
</ul>
</li>
<li>如何建树

<ul>
<li>是一种回归分类算法

<ul>
<li>“切分”和“解决”</li>
</ul>
</li>
<li>一开始的时候，所有的训练样本都在根部</li>
<li>然后分类的样本将基于选择的分类属性进行递归。</li>
</ul>
</li>
<li>理论上特征是可以重复选的</li>
<li>怎么找到比较好的树？

<ul>
<li>模型准确率高</li>
<li>找到尽可能最小的数来满足数据

<ul>
<li>分枝少</li>
<li>节点少</li>
</ul>
</li>
<li>occam&rsquo;s Racor：在效果超不多的时候，选比较简单的模型</li>
</ul>
</li>
<li>基本策略

<ul>
<li>贪心

<ul>
<li>当前把每个节点中的最优的特征作为分类标准</li>
<li>这样就不用穷举</li>
<li>评估

<ul>
<li>基尼系数（Gini index）</li>
<li>信息增益

<ul>
<li>用熵来衡量</li>
<li>混乱程度

<ul>
<li>如果容易分别出那个分类多，则混乱程度小</li>
<li>反之同理</li>
</ul>
</li>
<li>e.g 99/1（小） 50/50（大）</li>
</ul>
</li>
<li>则把每个叶子节点的熵都算出来

<ul>
<li>然后整棵树的熵则是用所有叶子节点的熵加权</li>
</ul>
</li>
<li>把分割前的熵减掉分割之后的，就是信息增益

<ul>
<li>如果比较大，那么就比较好</li>
</ul>
</li>
</ul>
</li>
<li> Gini index

<ul>
<li> 把叶子节点的熵换掉</li>
<li> 换成基尼系数</li>
<li> 基本上也是混乱度</li>
<li> 只是公式不一样了</li>
</ul>
</li>
<li> 整棵树的Gini系数是加权的分支的基尼</li>
<li> 如何防止overfitting？

<ul>
<li> 早一点停止树的增长</li>
<li> 先把所有节点来构造树，然后用的是90%的样本

<ul>
<li> 然后用剩下的样本来检验效果</li>
<li> 但是太慢了</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Bagging

<ul>
<li>构造k棵树</li>
<li>每棵树在1000各样本中随机选900个,构造决策树（只筛选样本）

<ul>
<li>然后把所有树的据结果统计，如果是分类问题，就少数服从多数。</li>
<li>随机选出的数据室可以重复的。</li>
</ul>
</li>
<li>如果是回归问题，则最终直接相加</li>
</ul>
</li>
<li><p>random forest</p>

<ul>
<li>不使用全部特征</li>
<li>不单只筛选样本，也筛选特征。</li>
<li>e.g 每次只用随机抽取的70%的样本，只用50%随机抽取的特征，来建立决策树</li>
<li>是目前分类回归中最好的off-the-shelf的算法

<ul>
<li>即拿即用</li>
</ul>
</li>
</ul>
</li>
<li><p>这两个算法加上boosting的不同在于训练集的构造的不同。</p></li>
<li>Boosting

<ul>
<li>结果是把每棵决策树的结果加权加起来</li>
<li>所有样本的权值加起来为1</li>
<li>对那些错的样本，权值是比较大的</li>
<li>缺点是在高噪声的环境下效果很不好

<ul>
<li>优点是在低噪声环境下效果很好</li>
</ul>
</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
</feed>
