<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Algorithm | KING]]></title>
  <link href="http://pbking1.github.com/blog/categories/algorithm/atom.xml" rel="self"/>
  <link href="http://pbking1.github.com/"/>
  <updated>2015-02-08T19:03:48-05:00</updated>
  <id>http://pbking1.github.com/</id>
  <author>
    <name><![CDATA[pb]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[KNN in Matlab(knnclassify)]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/26/knn-in-matlab-knnclassify/"/>
    <updated>2014-05-26T05:55:05-04:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/26/knn-in-matlab-knnclassify</id>
    <content type="html"><![CDATA[<ul>
<li>refer from

<ul>
<li><a href="http://blog.csdn.net/boyxiaolong/article/details/7062394">http://blog.csdn.net/boyxiaolong/article/details/7062394</a></li>
<li><a href="http://blog.csdn.net/aladdina/article/details/4141127">http://blog.csdn.net/aladdina/article/details/4141127</a></li>
</ul>
</li>
</ul>


<h3>What is KNN</h3>

<ul>
<li>simlpy, a method use to classify or regression</li>
<li>idea

<ul>
<li>如果一个样本在特征空间中的k个最相 似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。</li>
</ul>
</li>
<li>KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成正比。</li>
</ul>


<!--more-->


<h3><code>knnclassify</code> in matlab</h3>

<ul>
<li>sample
```
train_data = load(&lsquo;train.csv&rsquo;);
test_data = load(&lsquo;test.csv&rsquo;);</li>
</ul>


<p>X = train_data(:, 3:3074);
Y = train_data(:, 2);
x = test_data(:, 2:3073);</p>

<p>label = knnclassify(x,X,Y,10,&lsquo;cosine&rsquo;,&lsquo;random&rsquo;);
```
&ndash; knnclassify(test_data,train_data,train_label,numberoflabel,&lsquo;cosine&rsquo;,&lsquo;random&rsquo;);</p>

<h3>main function</h3>

<ul>
<li>1.CLASS = KNNCLASSIFY(SAMPLE,TRAINING,GROUP)

<ul>
<li>标号和训练数据必须有相同的行数；训练数据和测试数据必须有相同的列；函数对于无效值或者空值会作为丢失值或者忽略这一行。</li>
</ul>
</li>
<li>2.CLASS = KNNCLASSIFY(SAMPLE,TRAINING,GROUP,K)

<ul>
<li>此函数允许你设置距离矩阵形式，如：

<ul>
<li>&lsquo;euclidean&rsquo;    欧氏距离，默认的</li>
<li>&lsquo;cityblock&rsquo;    绝对差的和</li>
<li>&lsquo;cosine&rsquo;     角度距离</li>
<li>&lsquo;correlation&rsquo; 相关距离</li>
<li>&lsquo;Hamming&rsquo;      汉明距离</li>
</ul>
</li>
</ul>
</li>
<li>3.CLASS =KNNCLASSIFY(SAMPLE,TRAINING,GROUP,K,DISTANCE,RULE)

<ul>
<li>本函数允许你选择如何对样本进行分类，如你可以选择：

<ul>
<li>&lsquo;nearest&rsquo;  最近的K个的最多数</li>
<li>&lsquo;random&rsquo;    随机的最多数</li>
<li>&lsquo;consensus&rsquo; 投票法，默认的</li>
</ul>
</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Intro to Recommandation System]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/23/intro-to-recommandation-system/"/>
    <updated>2014-05-23T22:32:15-04:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/23/intro-to-recommandation-system</id>
    <content type="html"><![CDATA[<h3>Problem formulation</h3>

<pre><code>可能会有99%的数据是不知道的（用问号表示）
因此可以去猜那些不知道的数据是多少
然后把得分最高的电影推荐给他就可以了
所以其实是一个矩阵补全系统
</code></pre>

<!--more-->


<h3>基本思想和方法</h3>

<ul>
<li><p>基于人口统计学的推荐系统</p>

<ul>
<li>最简单的一种，只是根据系统用户的基本信息发现用户的相关程度，然后把相似用户喜爱的其他用品推荐给当前用户。</li>
<li>系统首先会根据用户的属性建模，比如用户的年龄，性别，兴趣等信息。根据这些特征计算用户间的相似度。比如系统通过计算发现用户A和C比较相似。就会把A喜欢的物品推荐给C。

<ul>
<li>优势

<ul>
<li>不需要历史数据，没有冷启动问题</li>
<li>不依赖于物品的属性，因此其他领域的问题都可无缝接入。</li>
</ul>
</li>
<li>不足：

<ul>
<li>算法比较粗糙，效果很难令人满意，只适合简单的推荐</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>content based recommender system(基于内容)</p>

<ul>
<li>使用物品本身的相似度而不是用户的相似度。</li>
<li>e.g 通过相似度计算，发现电影A和C相似度较高，因为他们都属于爱情类。系统还会发现用户A喜欢电影A，由此得出结论，用户A很可能对电影C也感兴趣。于是将电影C推荐给A。</li>
<li>假设有一些别的特征例如电影的浪漫成都或者动作成分含量（由专家弄出来的）</li>
<li>因此可以把原有的评分作为输入，把专家的预测作为y

<ul>
<li>做线性回归， 把每个用户的theta学出来</li>
<li>然后把theta和用户的评分做内积，得出那些问号的数据</li>
</ul>
</li>
</ul>
</li>
<li><p>协同过滤</p>

<ul>
<li>基于物品的协同过滤

<ul>
<li>根本思想是

<ul>
<li>预先根据所有用户的历史偏好数据计算物品之间的相似性，然后把和用户喜欢的物品相类似的物品推荐给用户。</li>
<li>假设a和c很相近，因为喜欢a的用户同时也喜欢c，而用户A喜欢a，所以把c推荐给用户A</li>
</ul>
</li>
</ul>
</li>
<li><p>如何计算相似度</p>

<ul>
<li>基于余弦的相似度计算

<ul>
<li>通过计算两个向量之间的夹角的余弦值来计算物品之间的相似性</li>
<li>公式为

<ul>
<li><img src="/images/recommander_system/1.png"></li>
</ul>
</li>
<li>这个算法也有改进版（修正的余弦相似性）

<ul>
<li>由于余弦相似度度量方法中没有考虑到不同用户的评分尺寸问题，修正的余弦相似性度量方法通过减去用户对项目的平均评分来改善上述缺陷。</li>
<li>公式为

<ul>
<li><img src="/images/recommander_system/2.png"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>基于关联的相似度计算

<ul>
<li>计算两个向量之间的Pearson-r关联度</li>
</ul>
</li>
<li>调整的余弦相似度计算

<ul>
<li>由于基于余弦的相似度计算没有考虑不同用户的打分情况，可能有的用户偏向于给低分，有的用户偏向于给高分，该方法通过减去用户的打分的平均值消除不同用户打分习惯的影响</li>
</ul>
</li>
</ul>
</li>
<li><p>预测值的计算</p>

<ul>
<li>根据之前计算好的物品之间的相似度，接下来对用户未打分的物品进行预测，有两种预测方法。

<ul>
<li>1.加权求和

<ul>
<li>通过公式

<ul>
<li><img src="/images/recommander_system/3.png"></li>
</ul>
</li>
</ul>
</li>
<li>2.回归</li>
</ul>
</li>
</ul>
</li>
<li><p>推荐系统的其中一种实现</p>

<ul>
<li>1.建立物品的同现矩阵

<ul>
<li>也就是按照用户分组，找出每两个物品再多少个用户中同时出现的次数</li>
</ul>
</li>
<li>2.建立用户的评分矩阵

<ul>
<li>也就是每个用户对每个物品的评分</li>
</ul>
</li>
<li>3.两个矩阵相乘，计算结果</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>源码</h3>

<ul>
<li>to be continued&hellip;</li>
</ul>


<h3>参考文献</h3>

<ul>
<li>《基于项目评分预测的协同过滤推荐算法》 邓爱林</li>
<li>相似度计算 <a href="http://blog.sina.com.cn/s/blog_7e11a6260101l9iq.html">http://blog.sina.com.cn/s/blog_7e11a6260101l9iq.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Perceptual Hash Algorithm in Objective C]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/19/perceptual-hash-algorithm-in-objective-c/"/>
    <updated>2014-05-19T05:47:06-04:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/19/perceptual-hash-algorithm-in-objective-c</id>
    <content type="html"><![CDATA[<h4>此文为借鉴阮一峰2011年和2013年发布的相似图片搜索原理</h4>

<ul>
<li>原文已经写得很好了，所以我只是把它整理了一下，学习学习~~</li>
</ul>


<h3>又名感知哈希算法</h3>

<ul>
<li>主要思想是

<ul>
<li>对每个图片生成一个“指纹”字符串，然后比较不同图片的指纹。结果越接近，就说明图片越相似</li>
</ul>
</li>
<li>这种算法的优点是简单快速，不收图片大小缩放的影响

<ul>
<li>缺点是图片内容不能变更。如果在图片上加几个文字，他就认不出来了</li>
</ul>
</li>
<li>因此最佳应用应该是根据缩略图找出原图</li>
</ul>


<!--more-->


<ul>
<li>算法样例

<ul>
<li>第一步 缩小尺寸

<ul>
<li>把图片缩小大8*8的尺寸，总共有64个像素。这一步的作用是去除图片的细节，只保留结构，明暗等基本信息，摒弃不同的尺寸，比例带来的图片差异</li>
</ul>
</li>
<li>第二步 简化色彩

<ul>
<li>将缩小之后的图片转为64级灰度。也就是说，所有的像素点总共之后64中颜色</li>
</ul>
</li>
<li>第三步 计算平均值

<ul>
<li>计算所有64个像素的灰度平均值</li>
</ul>
</li>
<li>第四步 比较像素的灰度

<ul>
<li>把每个像素的灰度，和平均值进行比较。大于或者等于平均值的，记为1；小于平均值，记为0；</li>
</ul>
</li>
<li>第五步 计算hash值

<ul>
<li>把上一步比较的结果，组合在一起，就构成一个64位的整数，这就是这张图片的指纹。组合的次序并不重要，只要保证所有图片采用同样的次序就行了。</li>
<li>hash_value = 127ysje82ewrdfw3(16个数字)</li>
<li>这个值也就是指纹</li>
<li>得到指纹之后就可以对比不同的图片，看看64为中有多少位是不一样的。理论上，这等同与计算”汉明距离“。如果不相同的数据位不超过5，这就说明两张图片很相似；如果大于10，就说明这是两张不同的图片。</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>网上其他两种类似的算法</h3>

<h4>颜色分布法</h4>

<ul>
<li>每张图片都可以生成颜色分布的直方图。如果两种那个图片的直方图很接近，那么就可以认为他们很相似</li>
<li>由于任何一种颜色都是有红绿蓝三原色（RGB）构成的，所以可以画出四幅图（三原色直方图和最后合成的直方图）</li>
<li>如果每种原色都可以取256个值，那么整个颜色空间共有1600万种颜色（256的三次方）。针对这1600万种颜色比较直方图，计算量实在太大了，因此需要采用简化方法。可以将0～255分成四个区：0～63为第0区，64～127为第1区，128～191为第2区，192～255为第3区。这意味着红绿蓝分别有4个区，总共可以构成64种组合（4的3次方）。</li>
<li>任何一种颜色必然属于这64种组合中的一种，这样就可以统计每一种组合包含的像素数量。</li>
<li><img src="/images/oc_algorithm1.png"></li>
<li>上图是某张图片的颜色分布表，将表中最后一栏提取出来，组成一个64维向量(7414, 230, 0, 0, 8, &hellip;, 109, 0, 0, 3415, 53929)。这个向量就是这张图片的特征值或者叫"指纹"。</li>
<li>于是，寻找相似图片就变成了找出与其最相似的向量。这可以用皮尔逊相关系数或者余弦相似度算出。</li>
</ul>


<h4>内容特征法</h4>

<ul>
<li>除了颜色构成还可以从比较图片内容的相似性入手</li>
<li>首先

<ul>
<li>把图片装成一张比较小的灰度图片，假设为50*50像素。然后，确定一个阀值，把灰度图片转成黑白图片</li>
</ul>
</li>
<li>其次

<ul>
<li>如果两张图片很相似，那么他们的黑白轮廓应该是相近的。因此，问题就变成了如何去顶一个合理的阀值，正确的呈现图片中的轮廓</li>
</ul>
</li>
<li>因此

<ul>
<li><strong>前景色和背景色反差越大，轮廓就越明显</strong></li>
<li>这意味着，如果我们找到一个值，可以使得前景色和背景色格子的“类内差异最小”，或者“类间差异最大”，那么这个值就是理想的阀值</li>
</ul>
</li>
<li>后来因为有个如本的学者叫大津展之证明了两个是一样的，可以用他的“大津法”来求阀值

<ul>
<li>假定一张图片共有n个像素，其中灰度值小于阈值的像素为 n1 个，大于等于阈值的像素为 n2 个（ n1 + n2 = n ）。w1 和 w2 表示这两种像素各自的比重。

<ul>
<li>w1 = n1 / n</li>
<li>w2 = n2 / n</li>
</ul>
</li>
<li>再假定，所有灰度值小于阈值的像素的平均值和方差分别为 μ1 和 σ1，所有灰度值大于等于阈值的像素的平均值和方差分别为 μ2 和 σ2。于是，可以得到

<ul>
<li>类内差异 = w1(σ1的平方) + w2(σ2的平方)
　　      &ndash; 类间差异 = w1w2(μ1-μ2)^</li>
</ul>
</li>
<li>可以证明，这两个式子是等价的：得到"类内差异"的最小值，等同于得到"类间差异"的最大值。不过，从计算难度看，后者的计算要容易一些。</li>
<li>下一步用"穷举法"，将阈值从灰度的最低值到最高值，依次取一遍，分别代入上面的算式。使得"类内差异最小"或"类间差异最大"的那个值，就是最终的阈值.</li>
<li><p>有了50x50像素的黑白缩略图，就等于有了一个50x50的0-1矩阵。矩阵的每个值对应原图的一个像素，0表示黑色，1表示白色。这个矩阵就是一张图片的特征矩阵。</p></li>
<li><p>两个特征矩阵的不同之处越少，就代表两张图片越相似。这可以用"异或运算"实现（即两个值之中只有一个为1，则运算结果为1，否则运算结果为0）。对不同图片的特征矩阵进行"异或运算"，结果中的1越少，就是越相似的图片。</p></li>
</ul>
</li>
</ul>


<h3>objective c源码</h3>

<ul>
<li>tphash.h</li>
</ul>


<p>```</p>

<h1>import &lt;Foundation/Foundation.h></h1>

<p>@interface tphash : NSObject</p>

<ul>
<li>(uint64_t)ptHash:(UIImage*)image;</li>
<li>(int)hamdistance:(uint64_t)x with:(uint64_t) y;</li>
<li>(UIImage <em>)scaleImage:(UIImage </em>)image toSize(CGSize)newSize;</li>
<li>(uint64_t <em>) convertTogreyscale64Array: (UIImage </em>)i;</li>
</ul>


<p>@end
```</p>

<ul>
<li>tphash.m</li>
</ul>


<p>```</p>

<h1>import &ldquo;tphash.h&rdquo;</h1>

<p>@implementation tphash</p>

<ul>
<li><p>(uint64_t)ptHash:(UIImage <em>)image{
  image = [self scaleImage:image toSize:CGSizeMake(8,8)];
  uint64_t</em> imageArray = [self convertTogreyscale64Array:image];
  int sum = 0;
  for(int i = 0; i &lt; 64; i++){
      sum += imageArray[i];
  }
  uint8_t avg = sum/64;
  uint64_t ret = 0;
  for(int i = 0; i &lt; 64; i++){
      if(imageArray[i] >= avg){
          ret++;
      }
      ret &lt;&lt;= 1;
  }
  return ret;
}</p></li>
<li><p>(int)hamdistance:(uint64_t)x with:(uint64_t) y{
  unsigned dist = 0, val = x<sup>y</sup>;
  while (val) {
      ++dist;
      val &amp;= val &ndash; 1;
  }
}</p></li>
<li><p>(UIImage <em>)scaleImage:(UIImage </em>)image toSize(CGSize)newSize{
  UIGraphicsBeginIMageContextWithOptions(newSize, NO, 0.0);
  [image drawInRect:CGRectMake(0,0,newSize.width, newSize.height)];
  UIImage *newImage = UIGraphicsBeginImageFromCurrentImageContext();
  UIGraphicsEndImageContext();
  return newImage;
}</p></li>
<li><p>(uint64_t <em>) convertTogreyscale64Array: (UIImage </em>)i{
  int kRed = 1;
  int kGreen = 2;
  int kBlue = 4;</p>

<p>  int colors = kGreen;
  int m_width = i.size.width;
  int m_height = i.size.height;</p>

<p>  uint32_t <em>rgbImage = (uint32_t </em>) malloc(m_width * meight * sizeof(uint32_t));
  CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
  CGContextRef context = CGBitmapContextCreate(rgbImage, m_width, m_height, 8, m_width * 4, colorSpace, kCGBitmapByteOrder32Little | kCGImageAlphaNoneSkipLast);
  CGContextSetInterpolationQuality(context, kCGInterpolationHigh);
  CGContextSetShouldAntialias(context, NO);
  CGContextDrawImage(context, CGRectMake(0, 0, m_width, m_height), [i CGImage]);
  CGContextRelease(context);
  CGColorSpaceRelease(colorSpace);</p>

<p>  uint8_t <em>m_imageData = (uint8_t </em>) malloc(m_width * m_height);
  for(int y = 0; y &lt; m_height; y++) {
      for(int x = 0; x &lt; m_width; x++) {
          uint32_t rgbPixel=rgbImage[y<em>m_width+x];
          uint32_t sum=0,count=0;
          if (colors &amp; kRed) {sum += (rgbPixel>>24)&255; count++;}
          if (colors &amp; kGreen) {sum += (rgbPixel>>16)&255; count++;}
          if (colors &amp; kBlue) {sum += (rgbPixel>>8)&255; count++;}
          m_imageData[y</em>m_width+x]=sum/count/4;
      }
  }
  free(rgbImage);
  return m_imageData;
}
@end</p></li>
</ul>


<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linear_regression_tutorial]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/12/linear-regression-tutorial/"/>
    <updated>2014-05-12T03:23:20-04:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/12/linear-regression-tutorial</id>
    <content type="html"><![CDATA[<h3>First something about training the data</h3>

<ul>
<li>Training set &ndash;> learning algorithm &ndash;> hypothesis &ndash;> estimated data</li>
<li>And we will use multiple features.

<ul>
<li><img src="/images/lr_gd/mfeat.png"></li>
</ul>
</li>
</ul>


<h3>What is linear regression</h3>

<ul>
<li>To say the idea in normal, we will have n feature</li>
<li>And what we are going to do is to observe them and suppose that they should be in a linear method.</li>
<li>And we should develop a way to find out the parameter to suit the hypothesis function.</li>
</ul>


<!--more-->


<h3>The first method to solve the problem(Gradient decend)</h3>

<h4>how it works?</h4>

<ul>
<li>There are several ideas

<ul>
<li><strong>learn rate</strong>

<ul>
<li>if too small, the gradient descent will be slow</li>
<li>if too large, the gradient descent will overshoot the minimum.

<ul>
<li>it may be fail to converge, or even diverge</li>
</ul>
</li>
<li><img src="/images/lr_gd/apha.png"></li>
</ul>
</li>
<li><strong>Cost function(J function)</strong>

<ul>
<li>The cost function has no relationship with the x and y.</li>
<li>and it all depend on the parameter <em>theta</em></li>
<li>and the &frac12; in the front is to make the derivation easier</li>
<li>And the cost function is used to <em>make the err(the err between hypothesis and real data) smaller</em>.</li>
</ul>
</li>
<li><strong>Hypothesis</strong>

<ul>
<li>This function is used to <em>make the function good</em>.</li>
<li>good enough to make the y calculate by the function can came close to the real y.</li>
<li>and we will use the err of them to see whether it is good enough or not.</li>
<li>and with all the data, we sum them.</li>
</ul>
</li>
<li><strong>theta</strong>

<ul>
<li><p>we can see in the following picture</p>

<ul>
<li>suppose that we went down the mountain and we can see all the mountain around us are higher than us</li>
<li>and the theta in the cost function is that we <em>reduce</em> theta value in the theta direction</li>
<li>and the deviation of the cost function on theta i is the distance that we move on theta i direction.</li>
</ul>
</li>
<li><p><img src="/images/lr_gd/gdpic.png"></p></li>
</ul>
</li>
</ul>
</li>
<li><strong>Basic algorithm</strong>

<ul>
<li>start with some theta0, theta1, theta2&hellip;..</li>
<li>Keep changing theta0, theta1, theta2&hellip;. to reduce J(theta0, theta2&hellip;) until we hopefully end up at a minimum.</li>
<li>And this is the key idea of Gradient descend method</li>
</ul>
</li>
<li><img src="/images/lr_gd/idea.png"></li>
</ul>


<h3>The cpp code of it</h3>

<p>```</p>

<h1>include&lt;stdlib.h></h1>

<h1>include&lt;stdio.h></h1>

<h1>include&lt;string.h></h1>

<h1>include&lt;math.h></h1>

<h1>define OUTPUTID 10001</h1>

<h1>define BUFFERSIZE 50000</h1>

<h1>define ROWNUM 10000</h1>

<h1>define COLNUM 385</h1>

<p>double alpha = 0.1;
char buffer[BUFFERSIZE];
const char *delim = &ldquo;,&rdquo;;
double x[ROWNUM][COLNUM];
double y[ROWNUM];
double result[ROWNUM];
double diff[ROWNUM];
double theta[COLNUM];
double temp[COLNUM];</p>

<p>void readdata(char <em>, bool);
void writedata(char </em>);
void test();
void gradient_descend_train();</p>

<p>int main(){</p>

<pre><code>readdata("train.csv", true);
gradient_descend_train();
readdata("test.csv", false);
test();
writedata("predict.csv");
return 0;
</code></pre>

<p>}</p>

<p>void readdata(char *filename, bool haspredicted){</p>

<pre><code>FILE *inputfile = fopen(filename, "r");

if(inputfile == NULL){
    system("PAUSE");
    exit(1);
}
//drop the first line
fscanf(inputfile, "%s", buffer);
//read all lines each
char *s;
for(int i = 0; i &lt; ROWNUM; i++){

    fscanf(inputfile, "%s", buffer);
    //drop the first column
    strtok(buffer, delim);
    //read the predict y
    if(haspredicted){
        s = strtok(NULL, delim);
        sscanf(s, "%lf", &amp;y[i]);
    }
    //init x0
    x[i][0] = 1;
    //read the matrix
    for(int j = 1; j &lt; COLNUM; j++){
        s = strtok(NULL, delim);
        sscanf(s, "%lf", &amp;x[i][j]);
    }
}
fclose(inputfile);
</code></pre>

<p>}</p>

<p>void writedata(char *filename){</p>

<pre><code>FILE *outputfile = fopen(filename, "w");

if(outputfile == NULL){
    system("pause");
    exit(1);
}

fprintf(outputfile, "%s,%s\n", "Id", "reference");
//write the result into file
for(int i = 0, id = OUTPUTID; i &lt; ROWNUM; i++, id++){
    //cout&lt;&lt;"write the line"&lt;&lt;i + 1&lt;&lt;endl;
    fprintf(outputfile, "%d,%.6lf\n", id, result[i]);
}
fclose(outputfile);
</code></pre>

<p>}</p>

<p>void initTheta(){  //init theta</p>

<pre><code>char *thetafilename = "theta.dat";
FILE *f = fopen(thetafilename, "r");
for(int j = 0; j &lt; COLNUM; j++)
    fscanf(f, "%lf", &amp;theta[j]);
fclose(f);
//init the theta
for(int j = 0; j &lt; COLNUM; j++)
    theta[j] = 0;
</code></pre>

<p>}</p>

<p>void saveTheta(){   //save the theta</p>

<pre><code>FILE *f = fopen("theta.dat", "w");
for(int j = 0; j &lt; COLNUM; j++)
    fprintf(f, "%lf\n", theta[j]);
fclose(f);
</code></pre>

<p>}</p>

<p>void calculateResult(){</p>

<pre><code>for(int i = 0; i &lt; ROWNUM; i++){
    result[i] = 0;
    for(int j = 0; j &lt; COLNUM; j++){
        result[i] += theta[j] * x[i][j];
    }
}
</code></pre>

<p>}</p>

<p>double calculateJ(){</p>

<pre><code>int turn = 0;
double cost = 0;
for(int i = 0; i &lt; ROWNUM; i++){
    diff[i] = result[i] - y[i];
    cost += diff[i]*diff[i];
}
cost /= (ROWNUM * 2);
printf("%5d: J(theta) = %.6lf\n", ++turn, cost);
return cost;
</code></pre>

<p>}</p>

<p>void updateTheta(){</p>

<pre><code>double sum;
for(int j = 0 ; j &lt; COLNUM; j++){
    sum = 0;
    for(int i = 0; i &lt; ROWNUM; i++)
        sum += diff[i] * x[i][j];
    theta[j] -= alpha * sum / ROWNUM;
}
</code></pre>

<p>}</p>

<p>void gradient_descend_train(){</p>

<pre><code>initTheta();
alpha = 0.1001;
double cost = 1000;
while(cost &gt; 26.4){
    calculateResult();
    cost = calculateJ();
    updateTheta();
}
saveTheta();
</code></pre>

<p>}</p>

<p>void test(){</p>

<pre><code>calculateResult();
</code></pre>

<p>}</p>

<p>```</p>

<h3>The Second method to solve the problem(normal equation)</h3>

<ul>
<li>This way is a way shown in the statistic learning.</li>
<li>use the minimum square function to do a regression analyse on the data.

<ul>
<li><img src="/images/lr_gd/nq.png"></li>
<li>and the process is shown : (385<em>10000)</em>(10000<em>385)</em>(385<em>10000)</em>10000<em>1=385</em>1</li>
</ul>
</li>
<li>and we can see the feature normalise in the normal equation function

<ul>
<li><img src="/images/lr_gd/nfn.png"></li>
</ul>
</li>
</ul>


<h3>The matlab code of it</h3>

<ul>
<li>The main function</li>
</ul>


<p>```
%load the train data
data = load(&lsquo;train.txt&rsquo;);
X = data(:, 3:386);
y = data(:, 2);
m = length(y);
m2 = size(X);</p>

<p>%load the test data
data2 = load(&lsquo;test.txt&rsquo;);
feat = data2(:, 2:385);
m3 = size(feat);</p>

<p>sum_test = [0];</p>

<p>%use the equation to calculate
theta = normaleqn(X, y, w);</p>

<p>%calculate the result
result = feat * theta;</p>

<p>csvwrite(&lsquo;aaa_ver3.csv&rsquo;, [linen result]);
```</p>

<ul>
<li>The normal equation function</li>
</ul>


<p>```
function [theta] = normaleqn(x, y, w)</p>

<pre><code>theta = zeros(size(x, 2), 1);
%theta = pinv(x' * x + 4000.3 * eye(size(x, 2))) * x' * y;
 %theta = pinv(x' * x + 3.3 * eye(size(x, 2))) * x' * y;
theta = pinv(x' * x + w * eye(size(x, 2))) * x' * y;
</code></pre>

<p>en
```</p>

<h3>The cpp code of it</h3>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tree Based Ensemble Method Introduction]]></title>
    <link href="http://pbking1.github.com/blog/2014/05/08/tree-based-ensemble-method-introduction/"/>
    <updated>2014-05-08T06:30:07-04:00</updated>
    <id>http://pbking1.github.com/blog/2014/05/08/tree-based-ensemble-method-introduction</id>
    <content type="html"><![CDATA[<h3>Tree-based ensemble methods</h3>

<pre><code>random forest
gradient boosted decision trees
</code></pre>

<ul>
<li>main idea is greedy algorithm

<ul>
<li>通过构造一棵决策树分类器</li>
</ul>
</li>
<li>随机森林是通过构造10000课树</li>
</ul>


<!--more-->


<h4>决策树</h4>

<ul>
<li>其实就是二叉树</li>
<li>实际应用，一般为二叉树</li>
<li>每个非叶子节点都是一个分割

<ul>
<li>相当于一个分类条件</li>
</ul>
</li>
<li>每个叶子节点都是一类性质相同的样本</li>
<li><p>用同一种数据存在多种构造决策树的情况</p></li>
<li><p>决策树使用来分类的模型</p>

<ul>
<li>相当于svn和LR</li>
</ul>
</li>
<li>如何建树

<ul>
<li>是一种回归分类算法

<ul>
<li>“切分”和“解决”</li>
</ul>
</li>
<li>一开始的时候，所有的训练样本都在根部</li>
<li>然后分类的样本将基于选择的分类属性进行递归。</li>
</ul>
</li>
<li>理论上特征是可以重复选的</li>
<li>怎么找到比较好的树？

<ul>
<li>模型准确率高</li>
<li>找到尽可能最小的数来满足数据

<ul>
<li>分枝少</li>
<li>节点少</li>
</ul>
</li>
<li>occam&rsquo;s Racor：在效果超不多的时候，选比较简单的模型</li>
</ul>
</li>
<li>基本策略

<ul>
<li>贪心

<ul>
<li>当前把每个节点中的最优的特征作为分类标准</li>
<li>这样就不用穷举</li>
<li>评估

<ul>
<li>基尼系数（Gini index）</li>
<li>信息增益

<ul>
<li>用熵来衡量</li>
<li>混乱程度

<ul>
<li>如果容易分别出那个分类多，则混乱程度小</li>
<li>反之同理</li>
</ul>
</li>
<li>e.g 99/1（小） 50/50（大）</li>
</ul>
</li>
<li>则把每个叶子节点的熵都算出来

<ul>
<li>然后整棵树的熵则是用所有叶子节点的熵加权</li>
</ul>
</li>
<li>把分割前的熵减掉分割之后的，就是信息增益

<ul>
<li>如果比较大，那么就比较好</li>
</ul>
</li>
</ul>
</li>
<li> Gini index

<ul>
<li> 把叶子节点的熵换掉</li>
<li> 换成基尼系数</li>
<li> 基本上也是混乱度</li>
<li> 只是公式不一样了</li>
</ul>
</li>
<li> 整棵树的Gini系数是加权的分支的基尼</li>
<li> 如何防止overfitting？

<ul>
<li> 早一点停止树的增长</li>
<li> 先把所有节点来构造树，然后用的是90%的样本

<ul>
<li> 然后用剩下的样本来检验效果</li>
<li> 但是太慢了</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Bagging

<ul>
<li>构造k棵树</li>
<li>每棵树在1000各样本中随机选900个,构造决策树（只筛选样本）

<ul>
<li>然后把所有树的据结果统计，如果是分类问题，就少数服从多数。</li>
<li>随机选出的数据室可以重复的。</li>
</ul>
</li>
<li>如果是回归问题，则最终直接相加</li>
</ul>
</li>
<li><p>random forest</p>

<ul>
<li>不使用全部特征</li>
<li>不单只筛选样本，也筛选特征。</li>
<li>e.g 每次只用随机抽取的70%的样本，只用50%随机抽取的特征，来建立决策树</li>
<li>是目前分类回归中最好的off-the-shelf的算法

<ul>
<li>即拿即用</li>
</ul>
</li>
</ul>
</li>
<li><p>这两个算法加上boosting的不同在于训练集的构造的不同。</p></li>
<li>Boosting

<ul>
<li>结果是把每棵决策树的结果加权加起来</li>
<li>所有样本的权值加起来为1</li>
<li>对那些错的样本，权值是比较大的</li>
<li>缺点是在高噪声的环境下效果很不好

<ul>
<li>优点是在低噪声环境下效果很好</li>
</ul>
</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
</feed>
